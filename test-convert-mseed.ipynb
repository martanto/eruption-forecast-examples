{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:29.987548900Z",
     "start_time": "2026-01-11T23:54:29.973412100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from obspy import read, Stream, UTCDateTime\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from obspy.signal.filter import bandpass\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from eruption_forecast.utils import detect_maximum_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95eb5fb7bc89bab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.010110900Z",
     "start_time": "2026-01-11T23:54:29.988060800Z"
    }
   },
   "outputs": [],
   "source": [
    "sds_dir = r\"D:\\Data\\OJN\"\n",
    "network = \"VG\"\n",
    "channel = \"EHZ\"\n",
    "station = \"OJN\"\n",
    "channel_type = \"D\"\n",
    "location = \"00\"\n",
    "start_datetime = \"2025-01-01\"\n",
    "end_datetime = \"2025-08-24\"\n",
    "extends_data = False\n",
    "tmp_dir = r\"D:\\Projects\\eruption-forecast\\output\\forecast\\VG.OJN.00.EHN\\tremor\\tmp\"\n",
    "overwrite = False\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1decad26a4e91b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.022125400Z",
     "start_time": "2026-01-11T23:54:30.011095800Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(tmp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28a02a980ad6c1bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.031616100Z",
     "start_time": "2026-01-11T23:54:30.024134800Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_bands = [(0.01, 0.1), (0.1, 2), (2, 5), (4.5, 8), (8, 16)]\n",
    "band_names = [\"rsam_vlf\", \"rsam_lf\", \"rsam\", \"rsam_mf\", \"rsam_hf\"]\n",
    "ratio_names = [\"dsar_vlf_lf\", \"dsar_lf_rsam\", \"dsar_rsam_mf\", \"dsar_mf_hf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f4fa124c09c3ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.038206200Z",
     "start_time": "2026-01-11T23:54:30.031616100Z"
    }
   },
   "outputs": [],
   "source": [
    "start_datetime_obj = datetime.strptime(start_datetime, \"%Y-%m-%d\")\n",
    "end_datetime_obj = datetime.strptime(end_datetime, \"%Y-%m-%d\")\n",
    "n_days = (end_datetime_obj - start_datetime_obj).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5a6b5b2ab2a9dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.047627Z",
     "start_time": "2026-01-11T23:54:30.038718300Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    [job_index, UTCDateTime(start_datetime_obj), \"OJN.EHN.VG.00\", extends_data]\n",
    "    for job_index in range(n_days)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d9872199cc21eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.054941Z",
     "start_time": "2026-01-11T23:54:30.048131700Z"
    }
   },
   "outputs": [],
   "source": [
    "def integrate(\n",
    "    data: np.ndarray,\n",
    "    sampling_rate: float = 100.0,\n",
    "    anchor_start_sample: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    displacements: np.ndarray = cumulative_trapezoid(\n",
    "        data, dx=1.0 / sampling_rate, initial=0\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"displacements[{anchor_start_sample}]: {displacements[anchor_start_sample]}\"\n",
    "        )\n",
    "\n",
    "    # anchoring to start_sample\n",
    "    if anchor_start_sample is not None:\n",
    "        displacements = displacements - displacements[anchor_start_sample]\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"after displacements[{anchor_start_sample}]: {displacements[anchor_start_sample]}\"\n",
    "            )\n",
    "\n",
    "    return displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95674db0b4217a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.064988500Z",
     "start_time": "2026-01-11T23:54:30.055581700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_mseed(utc_start_datetime: UTCDateTime, extends: bool = False) -> Stream:\n",
    "    stream = Stream()\n",
    "    extends = [-1, 0, 1] if extends else [0]\n",
    "\n",
    "    for extend in extends:\n",
    "        utc_datetime: UTCDateTime = utc_start_datetime + timedelta(days=extend)\n",
    "        year, julian_day = utc_datetime.format_seed().split(\",\")\n",
    "        filename = f\"{network}.{station}.{location}.{channel}.{channel_type}.{year}.{julian_day}\"\n",
    "\n",
    "        miniseed_file = os.path.join(\n",
    "            sds_dir,\n",
    "            str(year),\n",
    "            network,\n",
    "            station,\n",
    "            f\"{channel}.{channel_type}\",\n",
    "            filename,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(miniseed_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            stream = stream + read(miniseed_file, format=\"MSEED\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        stream = stream.merge(method=\"interpolate\")\n",
    "        if len(stream) > 0:\n",
    "            stream = stream.detrend(method=\"demean\")\n",
    "    except Exception as e:\n",
    "        return stream\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bbf8dfe8c34233b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.073585900Z",
     "start_time": "2026-01-11T23:54:30.065975100Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrapped_indices(\n",
    "    outlier_index, asymmetric_factor, subdomain_range, ten_minutes_samples\n",
    "):\n",
    "    asymmetric_factor_value = np.floor(asymmetric_factor * subdomain_range)\n",
    "    start_index = int(\n",
    "        outlier_index - asymmetric_factor_value\n",
    "    )  # Compute the index of the domain where the subdomain centered on the peak begins\n",
    "\n",
    "    end_index = start_index + subdomain_range  # Find the end index of the subdomain\n",
    "\n",
    "    if end_index >= ten_minutes_samples:  # If end index exceeds data range\n",
    "        index = list(\n",
    "            range(end_index - ten_minutes_samples)\n",
    "        )  # Wrap domain so continues from beginning of data range\n",
    "        end = list(range(start_index, ten_minutes_samples))\n",
    "        index.extend(end)\n",
    "    elif start_index < 0:  # If starting index exceeds data range\n",
    "        index = list(range(end_index))\n",
    "        end = list(\n",
    "            range(ten_minutes_samples + start_index, ten_minutes_samples)\n",
    "        )  # Wrap domains so continues at end of data range\n",
    "        index.extend(end)\n",
    "    else:\n",
    "        index = list(range(start_index, end_index))\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f\"outlier_index, asymmetric_factor_valus, start_index, end_index, total_windows, len(index): {outlier_index}, {asymmetric_factor_value}, {start_index}, {end_index}, {total_windows}, {len(index)}\")\n",
    "    # outlier_index, asymmetric_factor_valus, start_index, end_index, total_windows, len(index):\n",
    "    # 7932, 3.0, 7929, 7965, 144, 7821\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67ff81b2cffbe5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.088190800Z",
     "start_time": "2026-01-11T23:54:30.073585900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_for_day(\n",
    "    day_index, utc_start_datetime: UTCDateTime, _station, extends: bool = False\n",
    "):\n",
    "    # t0 = utc_datetime\n",
    "    # recalculate based on day_index\n",
    "    utc_start_datetime: UTCDateTime = utc_start_datetime + timedelta(days=day_index)\n",
    "    start_date_str = utc_start_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Load mseed\n",
    "    stream = load_mseed(utc_start_datetime, extends=extends)\n",
    "    trace = stream[0]\n",
    "    data = trace.data\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"utc_start_datetime, len_data: {utc_start_datetime}, {len(data)}\")\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    trace_starttime = trace.stats.starttime\n",
    "    sampling_rate = trace.stats.sampling_rate\n",
    "\n",
    "    # iO = start_sample\n",
    "    start_sample = utc_start_datetime - trace_starttime\n",
    "    start_sample = int(start_sample * sampling_rate)\n",
    "\n",
    "    # i1 = end_sample\n",
    "    end_sample = int(24 * 3600 * sampling_rate)\n",
    "    if (start_sample + end_sample) > len(data):\n",
    "        end_sample = len(data)\n",
    "    else:\n",
    "        end_sample = start_sample + end_sample\n",
    "    total_samples = end_sample - start_sample\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"day_index, start_sample, end_sample: {day_index}, ({start_sample} - {end_sample})\"\n",
    "        )\n",
    "\n",
    "    # N = ten_minutes_samples\n",
    "    # ten_minutes_samples = sampling_rate * 10 minutes * 60 seconds\n",
    "    ten_minutes_samples = int(10 * 60 * sampling_rate)\n",
    "\n",
    "    # m = total_windows\n",
    "    total_windows = int(total_samples // ten_minutes_samples)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"ten_minutes_samples, total_windows: {ten_minutes_samples}, {total_windows}\"\n",
    "        )\n",
    "\n",
    "    # integrating\n",
    "    displacement = integrate(\n",
    "        data, sampling_rate=sampling_rate, anchor_start_sample=start_sample\n",
    "    )\n",
    "\n",
    "    print(data[start_sample])\n",
    "\n",
    "    # save temporary results\n",
    "    datas = []\n",
    "    columns = []\n",
    "\n",
    "    # apply filter\n",
    "    all_data = []\n",
    "    all_displacement = []\n",
    "    for freq_min, freq_max in freq_bands:\n",
    "        _all_data = (\n",
    "            abs(\n",
    "                bandpass(data, freq_min, freq_max, sampling_rate)[\n",
    "                    start_sample:end_sample\n",
    "                ]\n",
    "            )\n",
    "            * 1.0e9\n",
    "        )\n",
    "        _all_displacement = (\n",
    "            abs(\n",
    "                bandpass(displacement, freq_min, freq_max, sampling_rate)[\n",
    "                    start_sample:end_sample\n",
    "                ]\n",
    "            )\n",
    "            * 1.0e9\n",
    "        )\n",
    "\n",
    "        all_data.append(_all_data)\n",
    "        all_displacement.append(_all_displacement)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"len(all_data[0]): {len(all_data[0])}\")\n",
    "\n",
    "    # find outliers\n",
    "    outliers = []\n",
    "    outlier_indices = []\n",
    "    for window_index in range(total_windows):\n",
    "        outlier, outlier_index, _ = detect_maximum_outlier(\n",
    "            all_data[2][\n",
    "                window_index * ten_minutes_samples : (window_index + 1)\n",
    "                * ten_minutes_samples\n",
    "            ]\n",
    "        )\n",
    "        outliers.append(outlier)\n",
    "        outlier_indices.append(outlier_index)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"len_outliers: {len(outliers)}\")\n",
    "\n",
    "    # calculate RSAM\n",
    "    asymmetric_factor = 0.1\n",
    "    number_subdomains = 4\n",
    "    subdomain_range = ten_minutes_samples // number_subdomains\n",
    "\n",
    "    for _all_data, band_name in zip(all_data, band_names):\n",
    "        rsam = []\n",
    "        rsam_without_outliers = []\n",
    "        for window_index, outlier, outlier_index in zip(\n",
    "            range(total_windows), outliers, outlier_indices\n",
    "        ):\n",
    "            _rsam = _all_data[\n",
    "                window_index * ten_minutes_samples : (window_index + 1)\n",
    "                * ten_minutes_samples\n",
    "            ]\n",
    "            rsam.append(np.mean(_rsam))\n",
    "\n",
    "            if outlier:\n",
    "                _outlier_index = wrapped_indices(\n",
    "                    outlier_index,\n",
    "                    asymmetric_factor,\n",
    "                    subdomain_range,\n",
    "                    ten_minutes_samples,\n",
    "                )\n",
    "                _rsam = np.delete(_rsam, _outlier_index)\n",
    "            rsam_without_outliers.append(np.mean(_rsam))\n",
    "\n",
    "        datas.append(np.array(rsam))\n",
    "        columns.append(band_name)\n",
    "\n",
    "        datas.append(np.array(rsam_without_outliers))\n",
    "        columns.append(f\"{band_name}_outlier\")\n",
    "\n",
    "    # calculate DSAR\n",
    "    for ratio_index, ratio_name in enumerate(ratio_names):\n",
    "        dsar = []\n",
    "        dsar_without_outliers = []\n",
    "        for window_index, outlier, outlier_index in zip(\n",
    "            range(total_windows), outliers, outlier_indices\n",
    "        ):\n",
    "            first_domain = all_displacement[ratio_index][\n",
    "                window_index * ten_minutes_samples : (window_index + 1)\n",
    "                * ten_minutes_samples\n",
    "            ]\n",
    "            second_domain = all_displacement[ratio_index + 1][\n",
    "                window_index * ten_minutes_samples : (window_index + 1)\n",
    "                * ten_minutes_samples\n",
    "            ]\n",
    "\n",
    "            dsar.append(np.mean(first_domain) / np.mean(second_domain))\n",
    "\n",
    "            if outlier:\n",
    "                _outlier_index = wrapped_indices(\n",
    "                    outlier_index,\n",
    "                    asymmetric_factor,\n",
    "                    subdomain_range,\n",
    "                    ten_minutes_samples,\n",
    "                )\n",
    "                first_domain = np.delete(first_domain, _outlier_index)\n",
    "                second_domain = np.delete(second_domain, _outlier_index)\n",
    "\n",
    "            dsar_without_outliers.append(np.mean(first_domain) / np.mean(second_domain))\n",
    "\n",
    "        datas.append(np.array(dsar))\n",
    "        columns.append(ratio_name)\n",
    "\n",
    "        datas.append(np.array(dsar_without_outliers))\n",
    "        columns.append(f\"{ratio_name}_outlier\")\n",
    "\n",
    "    datas = np.array(datas)\n",
    "    index = pd.date_range(\n",
    "        start_datetime_obj,\n",
    "        start_datetime_obj + timedelta(days=1),\n",
    "        freq=\"10min\",\n",
    "        inclusive=\"left\",\n",
    "    )\n",
    "    df = pd.DataFrame(zip(*datas), columns=columns, index=index)\n",
    "\n",
    "    filename = f\"{start_date_str}.csv\"\n",
    "    filepath = os.path.join(tmp_dir, filename)\n",
    "    df.to_csv(filepath, index=True)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2050487beec3ef9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:30.095029700Z",
     "start_time": "2026-01-11T23:54:30.089286Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    files = []\n",
    "    for job in jobs:\n",
    "        filepath = get_data_for_day(*job)\n",
    "        if filepath:\n",
    "            files.append(filepath)\n",
    "            print(filepath)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5462685f47925aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T23:54:33.703918900Z",
     "start_time": "2026-01-11T23:54:30.095572800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utc_start_datetime, len_data: 2025-01-01T00:00:00.000000Z, 8639901\n",
      "day_index, start_sample, end_sample: 0, (0 - 8639901)\n",
      "ten_minutes_samples, total_windows: 60000, 143\n",
      "displacements[0]: 0.0\n",
      "after displacements[0]: 0.0\n",
      "-128\n",
      "len(all_data[0]): 8639901\n",
      "len_outliers: 143\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (143) does not match length of index (144)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[59]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      2\u001B[39m files = []\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m job \u001B[38;5;129;01min\u001B[39;00m jobs:\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     filepath = \u001B[43mget_data_for_day\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mjob\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m filepath:\n\u001B[32m      6\u001B[39m         files.append(filepath)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[58]\u001B[39m\u001B[32m, line 128\u001B[39m, in \u001B[36mget_data_for_day\u001B[39m\u001B[34m(day_index, utc_start_datetime, _station, extends)\u001B[39m\n\u001B[32m    126\u001B[39m datas = np.array(datas)\n\u001B[32m    127\u001B[39m index = pd.date_range(start_datetime_obj, start_datetime_obj+timedelta(days=\u001B[32m1\u001B[39m), freq=\u001B[33m'\u001B[39m\u001B[33m10min\u001B[39m\u001B[33m'\u001B[39m, inclusive=\u001B[33m\"\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdatas\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    130\u001B[39m filename = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_date_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.csv\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    131\u001B[39m filepath = os.path.join(tmp_dir, filename)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\eruption-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:863\u001B[39m, in \u001B[36mDataFrame.__init__\u001B[39m\u001B[34m(self, data, index, columns, dtype, copy)\u001B[39m\n\u001B[32m    854\u001B[39m         columns = ensure_index(columns)\n\u001B[32m    855\u001B[39m     arrays, columns, index = nested_data_to_arrays(\n\u001B[32m    856\u001B[39m         \u001B[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;00m\n\u001B[32m    857\u001B[39m         \u001B[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    861\u001B[39m         dtype,\n\u001B[32m    862\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m863\u001B[39m     mgr = \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    864\u001B[39m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    865\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    869\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    871\u001B[39m     mgr = ndarray_to_mgr(\n\u001B[32m    872\u001B[39m         data,\n\u001B[32m    873\u001B[39m         index,\n\u001B[32m   (...)\u001B[39m\u001B[32m    877\u001B[39m         typ=manager,\n\u001B[32m    878\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\eruption-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001B[39m, in \u001B[36marrays_to_mgr\u001B[39m\u001B[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[39m\n\u001B[32m    116\u001B[39m         index = ensure_index(index)\n\u001B[32m    118\u001B[39m     \u001B[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     arrays, refs = \u001B[43m_homogenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# _homogenize ensures\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001B[39;00m\n\u001B[32m    122\u001B[39m     \u001B[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    125\u001B[39m \n\u001B[32m    126\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    127\u001B[39m     index = ensure_index(index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\eruption-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:630\u001B[39m, in \u001B[36m_homogenize\u001B[39m\u001B[34m(data, index, dtype)\u001B[39m\n\u001B[32m    627\u001B[39m         val = lib.fast_multiget(val, oindex._values, default=np.nan)\n\u001B[32m    629\u001B[39m     val = sanitize_array(val, index, dtype=dtype, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[43mcom\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequire_length_match\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    631\u001B[39m     refs.append(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    633\u001B[39m homogenized.append(val)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\eruption-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\common.py:573\u001B[39m, in \u001B[36mrequire_length_match\u001B[39m\u001B[34m(data, index)\u001B[39m\n\u001B[32m    569\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[33;03mCheck the length of data matches the length of the index.\u001B[39;00m\n\u001B[32m    571\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    572\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) != \u001B[38;5;28mlen\u001B[39m(index):\n\u001B[32m--> \u001B[39m\u001B[32m573\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    574\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mLength of values \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    575\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    576\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mdoes not match length of index \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    577\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(index)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    578\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Length of values (143) does not match length of index (144)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b8a2535743d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
